{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作家风格识别\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1实验背景\n",
    "\n",
    "作家风格是作家在作品中表现出来的独特的审美风貌。  \n",
    "通过分析作品的写作风格来识别作者这一研究有很多应用，比如可以帮助人们鉴定某些存在争议的文学作品的作者、判断文章是否剽窃他人作品等。  \n",
    "作者识别其实就是一个文本分类的过程，文本分类就是在给定的分类体系下，根据文本的内容自动地确定文本所关联的类别。\n",
    "写作风格学就是通过统计的方法来分析作者的写作风格，作者的写作风格是其在语言文字表达活动中的个人言语特征，是人格在语言活动中的某种体现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 实验要求\n",
    "a）建立深度神经网络模型，对一段文本信息进行检测识别出该文本对应的作者。   \n",
    "b）绘制深度神经网络模型图、绘制并分析学习曲线。  \n",
    "c）用准确率等指标对模型进行评估。    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 实验环境\n",
    "可以使用基于 Python 分词库进行文本分词处理，使用 Numpy 库进行相关数值运算，使用 Keras 等框架建立深度学习模型等。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 注意事项\n",
    "+ Python 与 Python Package 的使用方式，可在右侧 `API文档` 中查阅。\n",
    "+ 当右上角的『Python 3』长时间指示为运行中的时候，造成代码无法执行时，可以重新启动 Kernel 解决（左上角『Kernel』-『Restart Kernel』）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 参考资料\n",
    "jieba：https://github.com/fxsjy/jieba   \n",
    "Numpy：https://www.numpy.org/  \n",
    "Keras: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.1 介绍数据集\n",
    "\n",
    "该数据集包含了 8438 个经典中国文学作品片段，对应文件分别以作家姓名的首字母大写命名。  \n",
    "数据集中的作品片段分别取自 5 位作家的经典作品，分别是：\n",
    "\n",
    "|序号|中文名|英文名|文本片段个数|\n",
    "|--|--|--|--|\n",
    "|1|鲁迅|LX| 1500 条 |\n",
    "|2|莫言|MY| 2219 条 |\n",
    "|3|钱钟书|QZS| 1419 条 |\n",
    "|4|王小波|WXB| 1300 条 |\n",
    "|5|张爱玲|ZAL| 2000 条 |\n",
    "\n",
    "+ 其中截取的片段长度在 100~200 个中文字符不等\n",
    "+ 数据集路径为 \"./dataset/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba as jb\n",
    "import numpy as np\n",
    "import jieba.analyse\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集，保存在字典中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "path = \"./dataset/\"\n",
    "files= os.listdir(path)\n",
    "for file in files:\n",
    "    if not os.path.isdir(file) and not file[0] == '.': # 跳过隐藏文件和文件夹\n",
    "        f = open(path+\"/\"+file, 'r',  encoding='UTF-8'); # 打开文件\n",
    "        for line in f.readlines():\n",
    "            dataset[line] = file[:-4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集总共有 8438 个文本片段，现在展示其中的 10 个片段及其作者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_zh = {'LX': '鲁迅', 'MY':'莫言' , 'QZS':'钱钟书' ,'WXB':'王小波' ,'ZAL':'张爱玲'} \n",
    "for (k,v) in  list(dataset.items())[:10]:\n",
    "    print(k,'---',name_zh[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据集预处理  \n",
    "在做文本挖掘的时候，首先要做的预处理就是分词。  \n",
    "英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如 \"New York\" ，需要做为一个词看待。  \n",
    "而中文由于没有空格，分词就是一个需要专门去解决的问题了。  \n",
    "这里我们使用 jieba 包进行分词，使用**精确模式**、**全模式**和**搜索引擎模式**进行分词对比。  \n",
    "更多方法参考：https://github.com/fxsjy/jieba   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精确模式分词\n",
    "titles = [\".\".join(jb.cut(t, cut_all=False)) for t,_ in dataset.items()]\n",
    "print(\"精确模式分词结果:\\n\",titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全模式分词\n",
    "titles = [\".\".join(jb.cut(t, cut_all=True)) for t,_ in dataset.items()]\n",
    "print(\"全模式分词结果:\\n\",titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索引擎模式分词\n",
    "titles = [\".\".join(jb.cut_for_search(t)) for t,_ in dataset.items()]\n",
    "print(\"搜索引擎模式分词结果:\\n\",titles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 TF-IDF 算法统计各个作品的关键词频率\n",
    "TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索与文本挖掘的常用加权技术。  \n",
    "* TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。  \n",
    "* TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。  \n",
    "这里我们使用 jieba 中的默认语料库来进行关键词抽取，并展示每位作者前 5 个关键词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "# 将片段进行词频统计\n",
    "str_full = {}\n",
    "str_full['LX'] = \"\"\n",
    "str_full['MY'] = \"\"\n",
    "str_full['QZS'] = \"\"\n",
    "str_full['WXB'] = \"\"\n",
    "str_full['ZAL'] = \"\"\n",
    "\n",
    "for (k,v) in dataset.items():\n",
    "    str_full[v] += k\n",
    "\n",
    "for (k,v) in str_full.items():\n",
    "    print(k,\":\")\n",
    "    for x, w in jb.analyse.extract_tags(v, topK=5, withWeight=True):\n",
    "        print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 采用 Keras 建立一个简单的深度神经网络模型\n",
    "通过 Keras 构建深度学习模型的步骤如下：\n",
    "+ 定义模型——创建一个模型并添加配置层\n",
    "+ 编译模型——指定损失函数和优化器，并调用模型的 compile() 函数，完成模型编译。\n",
    "+ 训练模型——通过调用模型的 fit() 函数来训练模型。\n",
    "+ 模型预测——调用模型的 evaluate()或者 predict() 等函数对新数据进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 读取数据集\n",
    "首先需要读取数据集，记录每个片段的作者并保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据和标签\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    :param path:数据集文件夹路径\n",
    "    :return:返回读取的片段和对应的标签\n",
    "    \"\"\"\n",
    "    sentences = [] # 片段\n",
    "    target = [] # 作者\n",
    "    \n",
    "    # 定义lebel到数字的映射关系\n",
    "    labels = {'LX': 0, 'MY': 1, 'QZS': 2, 'WXB': 3, 'ZAL': 4}\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            f = open(path + \"/\" + file, 'r', encoding='UTF-8');  # 打开文件\n",
    "            for line in f.readlines():\n",
    "                sentences.append(line)\n",
    "                target.append(labels[file[:-4]])\n",
    "                \n",
    "    target = np.array(target)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(target)\n",
    "    encoded_target = encoder.transform(target)\n",
    "    dummy_target = to_categorical(encoded_target)\n",
    "\n",
    "    return sentences, dummy_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对读取的片段进行分词，由于分词后的片段任然为中文词语组成的序列，需要创建词汇表，将每个中文词映射为一个数字。这里使用 tf 的 Tokenizer 创建词汇表。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(text_processed,path, max_sequence_length=80):\n",
    "    \"\"\"\n",
    "    数据处理，如果使用 lstm，则可以接收不同长度的序列。\n",
    "    :text_processed：不定长的 Token 化文本序列，二维list\n",
    "    :path：数据集路径\n",
    "    :max_sequence_length：padding 大小，长句截断短句补 0\n",
    "    :return 处理后的序列，numpy 格式的二维数组\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for text in text_processed:\n",
    "        if len(text) > max_sequence_length:\n",
    "            text = text[:max_sequence_length]\n",
    "        else:\n",
    "            text = text + [0 for i in range(max_sequence_length-len(text))]\n",
    "        res.append(text)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看我们创建词汇表的结果\n",
    "sentences,target = load_data(path)\n",
    "\n",
    "#定义是文档的最大长度。如果文本的长度大于最大长度，那么它会被剪切，反之则用0填充\n",
    "max_sequence_length = 80\n",
    "\n",
    "# 使用 jieba 机精确模式分词\n",
    "sentences = [\".\".join(jb.cut(t, cut_all=False)) for t in sentences]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer** 类允许使用两种方法向量化一个文本语料库： 将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）； 或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF-IDF权重等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "vocab_processor = tf.keras.preprocessing.text.Tokenizer(num_words=60000,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', oov_token='<UNK>')\n",
    "# 要用以训练的文本列表\n",
    "vocab_processor.fit_on_texts(sentences)\n",
    "# 序列的列表，将 sentences 文本序列化\n",
    "text_processed = vocab_processor.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将词汇表保存为 json，后续可以直接读取，读取方式为 **tf.keras.preprocessing.text.tokenizer_from_json(json_string)**，可以获得vocab_processor 相同参数的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_json_string = vocab_processor.to_json()\n",
    "# 将词汇表保存路径\n",
    "vocab_keras_path = \"results/vocab_keras.json\"\n",
    "file = open(vocab_keras_path,\"w\")\n",
    "file.write(vocab_json_string)\n",
    "file.close()\n",
    "\n",
    "# 将句子 padding 为固定长度，如果使用lstm则不需要 padding 为固定长度\n",
    "text_processed = padding(text_processed,path)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打乱并切分数据集，取 30% 数据作为验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集比例\n",
    "val_split = 0.3\n",
    "\n",
    "# 打乱顺序\n",
    "text_target = list(zip(text_processed, target))\n",
    "random.shuffle(text_target)\n",
    "text_processed[:], target[:] = zip(*text_target)\n",
    "\n",
    "# 验证集数目\n",
    "val_counts = int(val_split*len(text_target))\n",
    "\n",
    "# 切分验证集\n",
    "val_X = text_processed[-val_counts:]\n",
    "val_y = target[-val_counts:]\n",
    "train_X = text_processed[:-val_counts]\n",
    "train_y = target[:-val_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 常见的创建模型方式介绍\n",
    "Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 [Sequential 顺序模型](https://keras.io/getting-started/sequential-model-guide/)，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。下面先来看看 Sequential 顺序模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式一: 使用 .add() 方法将各层添加到模型中\n",
    "# 导入相关包\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# 选择模型，选择序贯模型（Sequential())\n",
    "model = Sequential()\n",
    "\n",
    "# 构建网络层\n",
    "# 添加全连接层\n",
    "model.add(Dense(64, input_shape=(max_sequence_length,)))\n",
    "\n",
    "# 添加激活层，激活函数是 relu\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式二：网络层实例的列表构建序贯模型\n",
    "# 导入相关的包\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# 选择模型，选择序贯模型（Sequential())\n",
    "# 通过将网络层实例的列表传递给 Sequential 的构造器，来创建一个 Sequential 模型\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(max_sequence_length,)),\n",
    "    Activation('relu')\n",
    "])\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式三：函数式模型\n",
    "# 导入相关的包\n",
    "from tensorflow.keras.layers import Input, Dense,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 输入层，返回一个张量 tensor\n",
    "inputs = Input(shape=(max_sequence_length,))\n",
    "\n",
    "# 全连接层，返回一个张量\n",
    "output_1 = Dense(64)(inputs)\n",
    "\n",
    "# 激活函数层\n",
    "predictions= Activation(activation='relu')(output_1)\n",
    "\n",
    "# 创建一个模型，包含输入层、全连接层和激活层\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 建立深度学习模型\n",
    "\n",
    "+ 创建一个简单的深度接神经网络模型用于文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(train_X, train_y, val_X, val_y, model_save_path='results/demo.h5',\n",
    "              log_dir=\"results/logs/\"):\n",
    "\n",
    "    # 选择模型，选择序贯模型（Sequential())\n",
    "    model = K.Sequential()\n",
    "    \n",
    "    # 构建网络层\n",
    "    # 添加全连接层，输出空间维度64\n",
    "    model.add(K.layers.Dense(64))\n",
    "\n",
    "    # 添加激活层，激活函数是 relu\n",
    "    model.add(K.layers.Activation('relu'))\n",
    "    model.add(K.layers.Dense(5))\n",
    "    model.add(K.layers.Activation(\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    # 训练模型\n",
    "    history = model.fit(train_X.astype(np.float64), train_y, batch_size=128, epochs=5,validation_data=(val_X, val_y) )\n",
    "\n",
    "    # 保存模型\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型训练过程和模型概况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始时间\n",
    "start = time.time()\n",
    "\n",
    "# 数据预处理\n",
    "data_path = \"./dataset/\"\n",
    "\n",
    "# 训练模型，获取训练过程和训练后的模型\n",
    "history,model = dnn_model(train_X, train_y, val_X, val_y)\n",
    "\n",
    "# 打印模型概况和模型训练总数长\n",
    "model.summary()\n",
    "print(\"模型训练总时长：\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型训练过程图形化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(res):\n",
    "    \"\"\"\n",
    "    绘制模型的训练结果\n",
    "    :param res: 模型的训练结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 绘制模型训练过程的损失和平均损失\n",
    "    # 绘制模型训练过程的损失值曲线，标签是 loss\n",
    "    plt.plot(res.history['loss'], label='loss')\n",
    "    \n",
    "    # 绘制模型训练过程中的平均损失曲线，标签是 val_loss\n",
    "    plt.plot(res.history['val_loss'], label='val_loss')\n",
    "    \n",
    "    # 绘制图例,展示出每个数据对应的图像名称和图例的放置位置\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # 展示图片\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制模型训练过程中的的准确率和平均准确率\n",
    "    # 绘制模型训练过程中的准确率曲线，标签是 acc\n",
    "    plt.plot(res.history['accuracy'], label='accuracy')\n",
    "    \n",
    "    # 绘制模型训练过程中的平均准确率曲线，标签是 val_acc\n",
    "    plt.plot(res.history['val_accuracy'], label='val_accuracy')\n",
    "    \n",
    "    # 绘制图例,展示出每个数据对应的图像名称，图例的放置位置为默认值。\n",
    "    plt.legend()\n",
    "    \n",
    "    # 展示图片\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制模型训练过程曲线\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 加载模型和模型评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_model_prediction(val_X, val_y, model_path = 'results/demo.h5'):\n",
    "    \"\"\"\n",
    "    加载模型和模型评估，打印验证集的 loss 和准确度\n",
    "    :param validation_generator: 预测数据\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model = K.models.load_model(model_path)\n",
    "    # 获取验证集的 loss 和 accuracy\n",
    "    loss, accuracy = model.evaluate(val_X, val_y)\n",
    "    print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_model_prediction(val_X, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 加载模型并进行预测  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载demo模型\n",
    "model = load_model(\"results/demo.h5\")\n",
    "\n",
    "# 使用阿Q正传的片段测试\n",
    "sen = \"阿Q住在未庄的土谷祠里，给人家打短工度日。\\\n",
    "       虽然常常被村里人开玩笑，但内心他还反过来看不起村里人。\\\n",
    "       他有一个缺点，就是头上有一块癞疮疤。所以只要被人说道有关疮疤的话题，他就发怒。\\\n",
    "       大家觉得他的发怒很有趣，就更加开他的玩笑了。如果觉得对手弱，他就故意找茬吵架。但结果往往是输。\"\n",
    "\n",
    "# 对输入文本进行预处理，使用同一个词汇表进行文本向量化\n",
    "sen_prosessed = \" \".join(jb.cut(sen, cut_all=True))\n",
    "sen_prosessed = vocab_processor.texts_to_sequences([sen_prosessed])[0]\n",
    "sen_prosessed = sen_prosessed[:max_sequence_length] if len(sen_prosessed) > max_sequence_length else sen_prosessed + [0 for i in range(max_sequence_length - len(sen_prosessed))]\n",
    "sen_prosessed = np.array(sen_prosessed).reshape(1,-1)\n",
    "\n",
    "# 输入模型进行预测\n",
    "result = model.predict(sen_prosessed)\n",
    "max_sequence_length\n",
    "catalogue = list(result[0]).index(max(result[0]))\n",
    "\n",
    "# 定义int2string的映射关系\n",
    "labels = {0: '鲁迅', 1: '莫言', 2: '钱钟书', 3: '王小波', 4: '张爱玲'}\n",
    "\n",
    "print(\"这是{}的文章\".format(labels[catalogue]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.作业\n",
    "**作业内容：** 根据一段中文文本（ 100~200 个中文字符），预测这段文本的作者。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 创建并训练模型\n",
    "深度学习模型训练流程, 包含数据处理、创建模型、训练模型、模型保存、评价模型等。  \n",
    "如果对训练出来的模型不满意, 你可以通过调整模型的参数等方法重新训练模型, 直至训练出你满意的模型。  \n",
    "如果你对自己训练出来的模型非常满意, 则可以提交作业!  \n",
    "可以使用任意一种平台支持的框架进行模型实现。\n",
    "\n",
    "注意：\n",
    "\n",
    "1. 你可以在我们准好的接口中实现深度学习模型（若使用可以修改除predict外的函数接口），也可以自己实现深度学习模型或改用其他框架实现,但需要满足predict函数的输入输出符合格式要求！\n",
    "2. 写好代码后可以在 Py 文件中使用 GPU 进行模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================实现自己的深度学习模型代码答题区===========================================\n",
    "\n",
    "双击下方区域开始编写  **数据处理**、**创建模型**、**训练模型**、**保存模型**  和  **评估模型**  等部分的代码，如果使用其他框架训练模型可以自行实现。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend  as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,GRU\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "def processing_data(data_path, validation_split = 0.3 ):\n",
    "    \"\"\"\n",
    "    数据处理\n",
    "    :data_path：数据集路径\n",
    "    :validation_split：划分为验证集的比重\n",
    "    :return：train_X,train_y,val_X,val_y \n",
    "    \"\"\"\n",
    "    # --------------- 在这里实现中文文本预处理，包含分词，建立词汇表等步骤 -------------------------------\n",
    "    train_X, train_y, test_X, test_y = None,None,None,None\n",
    "    pass\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def model(train_X, train_y, val_X, val_y, save_model_path):\n",
    "    \"\"\"\n",
    "    创建、训练和保存深度学习模型\n",
    "    :param train_X: 训练集特征\n",
    "    :param train_y: 训练集target\n",
    "    :param test_X: 测试集特征\n",
    "    :param test_y: 测试集target\n",
    "    :param save_model_path: 保存模型的路径和名称\n",
    "    \"\"\"\n",
    "    # --------------------- 实现模型创建、训练和保存等部分的代码 --------------------- \n",
    "    pass\n",
    "    # 保存模型（请写好保存模型的路径及名称）\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return \n",
    "\n",
    "def evaluate_mode(val_X, val_y, save_model_path):\n",
    "    \"\"\"\n",
    "    加载模型和评估模型\n",
    "    可以实现，比如: 模型训练过程中的学习曲线，测试集数据的loss值、准确率及混淆矩阵等评价指标！\n",
    "    主要步骤:\n",
    "        1.加载模型(请填写你训练好的最佳模型),\n",
    "        2.对自己训练的模型进行评估\n",
    "\n",
    "    :param test_X: 测试集特征\n",
    "    :param test_y: 测试集target\n",
    "    :param save_model_path: 加载模型的路径和名称,请填写你认为最好的模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ----------------------- 实现模型加载和评估等部分的代码 -----------------------\n",
    "    pass\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    深度学习模型训练流程,包含数据处理、创建模型、训练模型、模型保存、评价模型等。\n",
    "    如果对训练出来的模型不满意,你可以通过调整模型的参数等方法重新训练模型,直至训练出你满意的模型。\n",
    "    如果你对自己训练出来的模型非常满意,则可以提交作业!\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_path = \"./dataset\"  # 数据集路径\n",
    "    save_model_path = \"results/model.h5\"  # 保存模型路径和名称\n",
    "    validation_split = 0.2 #验证集比重\n",
    "\n",
    "    # 获取数据、并进行预处理\n",
    "    train_X, train_y, val_X, val_y = processing_data(data_path, validation_split = validation_split)\n",
    "\n",
    "    # 创建、训练和保存模型\n",
    "    model(train_X, train_y, val_X, val_y, save_model_path)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate_mode(val_X, val_y, save_model_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 模型预测\n",
    "\n",
    "\n",
    "注意：\n",
    "1. 点击左侧栏`提交结果`后点击`生成文件`则只需勾选 `predict()` 函数的cell，即【**模型预测代码答题区域**】的 cell。\n",
    "2. 请导入必要的包和第三方库 (包括此文件中曾经导入过的)。\n",
    "3. 请加载你认为训练最佳的模型，即请按要求填写模型路径。\n",
    "4. `predict()`函数的输入和输出请不要改动。\n",
    "5. 作业测试时记得填写你的模型路径及名称, 如果采用 [离线任务](https://momodel.cn/docs/#/zh-cn/%E5%9C%A8GPU%E6%88%96CPU%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) 请将模型保存在 **results** 文件夹下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================  **提交 Notebook 训练模型结果数据处理参考示范**  =================================\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "select": true
   },
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import numpy as np\n",
    "import jieba as jb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 本 cell 代码仅为 Notebook 训练模型结果进行平台测试代码示范\n",
    "# 可以实现个人数据处理的方式，平台测试通过即可提交代码\n",
    "#  -----------------------------------------------------------------------------\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    读取数据和标签\n",
    "    :param path:数据集文件夹路径\n",
    "    :return:返回读取的片段和对应的标签\n",
    "    \"\"\"\n",
    "    sentences = []  # 片段\n",
    "    target = []  # 作者\n",
    "\n",
    "    # 定义lebel到数字的映射关系\n",
    "    labels = {'LX': 0, 'MY': 1, 'QZS': 2, 'WXB': 3, 'ZAL': 4}\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            f = open(path + \"/\" + file, 'r', encoding='UTF-8');  # 打开文件\n",
    "            for line in f.readlines():\n",
    "                sentences.append(line)\n",
    "                target.append(labels[file[:-4]])\n",
    "\n",
    "    target = np.array(target)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(target)\n",
    "    encoded_target = encoder.transform(target)\n",
    "    dummy_target = to_categorical(encoded_target)\n",
    "\n",
    "    return sentences, dummy_target\n",
    "\n",
    "\n",
    "def padding(text_processed, path, max_sequence_length=80):\n",
    "    \"\"\"\n",
    "    数据处理，如果使用 lstm，则可以接收不同长度的序列。\n",
    "    :text_processed：不定长的 Token 化文本序列，二维list\n",
    "    :path：数据集路径\n",
    "    :max_sequence_length：padding 大小，长句截断短句补 0\n",
    "    :return 处理后的序列，numpy 格式的二维数组\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for text in text_processed:\n",
    "        if len(text) > max_sequence_length:\n",
    "            text = text[:max_sequence_length]\n",
    "        else:\n",
    "            text = text + [0 for i in range(max_sequence_length - len(text))]\n",
    "        res.append(text)\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "path = \"./dataset/\"\n",
    "sentences, target = load_data(path)\n",
    "max_sequence_length = 80\n",
    "# 使用jieba机精确模式分词\n",
    "sentences = [\".\".join(jb.cut(t, cut_all=False)) for t in sentences]\n",
    "vocab_processor = tf.keras.preprocessing.text.Tokenizer(num_words=60000, oov_token='<UNK>')\n",
    "vocab_processor.fit_on_texts(sentences)\n",
    "text_processed = vocab_processor.texts_to_sequences(sentences)\n",
    "# 将句子padding为固定长度，如果使用lstm则不需要padding为固定长度\n",
    "text_processed = padding(text_processed, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================  **模型预测代码答题区域**  =========================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下方的代码块中编写 **模型预测** 部分的代码，请勿在别的位置作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "select": true
   },
   "outputs": [],
   "source": [
    "# -------------------------- 请加载您最满意的模型 ---------------------------\n",
    "# 加载模型(请加载你认为的最佳模型)\n",
    "# 加载模型,加载请注意 model_path 是相对路径, 与当前文件同级。\n",
    "# 如果你的模型是在 results 文件夹下的 demo.h5 模型，则 model_path = 'results/demo.h5'\n",
    "model_path = None\n",
    "\n",
    "# 加载模型，如果采用 keras 框架训练模型，则 model=load_model(model_path)\n",
    "model = load_model(model_path)\n",
    "\n",
    "# ---------------------- 模型预测，注意不要修改函数的输入输出 -------------------------\n",
    "def predict(text):\n",
    "    \"\"\"\n",
    "    :param text: 中文字符串\n",
    "    :return: 字符串格式的作者名缩写，比如:'LX'\n",
    "    \"\"\"\n",
    "    # ----------- 实现预测部分的代码，以下样例可代码自行删除，实现自己的处理方式 -----------\n",
    "    \n",
    "    labels = {0: 'LX', 1: 'MY', 2: 'QZS', 3: 'WXB', 4: 'ZAL'}\n",
    "    sen_prosessed = \" \".join(jb.cut(text, cut_all=True))\n",
    "    max_sequence_length = 80\n",
    "    sen_prosessed = vocab_processor.texts_to_sequences([sen_prosessed])[0]\n",
    "    if len(sen_prosessed) > max_sequence_length:\n",
    "        sen_prosessed = sen_prosessed[:max_sequence_length]\n",
    "    else:\n",
    "        sen_prosessed += [0 for _ in range(max_sequence_length - len(sen_prosessed))]\n",
    "    sen_prosessed = np.array(sen_prosessed).reshape(1, -1)\n",
    "    \n",
    "    # 加载模型进行预测\n",
    "    result = model.predict(sen_prosessed)\n",
    "    prediction = labels[list(result[0]).index(max(result[0]))]\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"我听到一声尖叫，感觉到蹄爪戳在了一个富有弹性的东西上。定睛一看，不由怒火中烧。原来，趁着我不在，隔壁那个野杂种——沂蒙山猪刁小三，正舒坦地趴在我的绣榻上睡觉。我的身体顿时痒了起来，我的目光顿时凶了起来。\"\n",
    "predict(sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
